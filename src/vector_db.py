"""
Vector Database modÃ¼lÃ¼
ChromaDB ile embedding'leri saklar ve arar
"""

import chromadb # type: ignore
from chromadb.config import Settings # type: ignore
from typing import List, Dict
import numpy as np # type: ignore
from config import config


class VectorDB:
    """ChromaDB wrapper sÄ±nÄ±fÄ±"""
    
    def __init__(self, collection_name: str = "dnd_knowledge"):
        """
        Args:
            collection_name: Koleksiyon adÄ± (veritabanÄ± tablosu gibi)
        """
        # ChromaDB client oluÅŸtur (persistent storage)
        self.client = chromadb.PersistentClient(
            path=str(config.VECTOR_DB_DIR)
        )
        
        # Collection oluÅŸtur veya mevcut olanÄ± getir
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "D&D 5e knowledge base"}
        )
        
        print(f"âœ… ChromaDB hazÄ±r: {collection_name}")
        print(f"ğŸ“Š Mevcut document sayÄ±sÄ±: {self.collection.count()}")
    
    def add_documents(self, documents: List[Dict]):
        """
        Embedding'li document'larÄ± database'e ekle
        
        Args:
            documents: embedder'dan gelen documents (embedding field'Ä± olmalÄ±)
        """
        if not documents:
            print("âš ï¸ Eklenecek document yok!")
            return
        
        # ChromaDB formatÄ±na Ã§evir
        ids = [f"doc_{i}" for i in range(len(documents))]
        texts = [doc['text'] for doc in documents]
        embeddings = [doc['embedding'].tolist() for doc in documents]
        metadatas = [doc['metadata'] for doc in documents]
        
        print(f"ğŸ’¾ {len(documents)} document database'e ekleniyor...")
        
        # Batch olarak ekle (ChromaDB 5000'lik batch'leri sever)
        batch_size = 1000
        for i in range(0, len(documents), batch_size):
            end_idx = min(i + batch_size, len(documents))
            
            self.collection.add(
                ids=ids[i:end_idx],
                documents=texts[i:end_idx],
                embeddings=embeddings[i:end_idx],
                metadatas=metadatas[i:end_idx]
            )
            
            print(f"   âœ… {end_idx}/{len(documents)} eklendi")
        
        print(f"âœ… Toplam {self.collection.count()} document database'de")
    
    def search(self, query_text: str, n_results: int = 5, query_embedding: np.ndarray = None) -> List[Dict]:
        """
        Query text'ine benzer document'larÄ± ara
        
        Args:
            query_text: Aranacak text
            n_results: KaÃ§ sonuÃ§ dÃ¶ndÃ¼rÃ¼lsÃ¼n (top-k)
            query_embedding: Ã–nceden hazÄ±rlanmÄ±ÅŸ query embedding (opsiyonel)
            
        Returns:
            En benzer document'larÄ±n listesi
        """
        # EÄŸer embedding verilmiÅŸse onu kullan
        if query_embedding is not None:
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=n_results
            )
        else:
            # Text ile ara (ChromaDB kendi embedding'ini kullanÄ±r)
            results = self.collection.query(
                query_texts=[query_text],
                n_results=n_results
            )
        
        # Format results
        formatted_results = []
        for i in range(len(results['ids'][0])):
            # Distance'Ä± similarity'ye Ã§evir
            distance = results['distances'][0][i] if 'distances' in results else None
            similarity = None
            if distance is not None:
                # ChromaDB L2 distance kullanÄ±r, 0-2 arasÄ± normalize edelim
                similarity = max(0, 1 - (distance / 2))
            
            doc = {
                'id': results['ids'][0][i],
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': distance,
                'similarity': similarity
            }
            formatted_results.append(doc)
        
        return formatted_results
    
    def clear(self):
        """Database'i temizle"""
        self.client.delete_collection(self.collection.name)
        print(f"ğŸ—‘ï¸ Collection '{self.collection.name}' silindi")
    
    def get_stats(self) -> Dict:
        """Database istatistikleri"""
        return {
            "collection_name": self.collection.name,
            "document_count": self.collection.count(),
            "storage_path": str(config.VECTOR_DB_DIR)
        }


def main():
    """Test scripti"""
    from text_chunker import TextChunker
    from embedder import Embedder
    from pdf_processor import extract_text_from_pdf, list_pdfs
    
    print("="*60)
    print("VECTOR DATABASE TEST")
    print("="*60 + "\n")
    
    # PDF'leri listele
    pdfs = list_pdfs()
    
    if not pdfs:
        print("âŒ PDF bulunamadÄ±!")
        return
    
    pdf_path = pdfs[0]
    print(f"ğŸ“„ Test PDF: {pdf_path.name}\n")
    
    # Text iÅŸle
    print("ğŸ“– Text iÅŸleniyor...")
    text = extract_text_from_pdf(pdf_path)
    
    chunker = TextChunker()
    chunks = chunker.chunk_text(text, source_name=pdf_path.name)
    
    # GerÃ§ek iÃ§erik iÃ§in 100-300 arasÄ± chunk'lar (ilk 100 genelde Contents/Intro)
    test_chunks = chunks  # TÃœM CHUNK'LAR (~2900)
    print(f"âœ… {len(test_chunks)} chunk hazÄ±r (TÃœM PDF)\n")
    
    # DOÄRU MODEL ile embedding (all-mpnet-base-v2, 768-dim)
    print("ğŸ”¤ Embedding modeli yÃ¼kleniyor...")
    embedder = Embedder(model_name="all-mpnet-base-v2")
    embedded_docs = embedder.embed_documents(test_chunks)
    print()
    
    # Vector DB'ye kaydet
    db = VectorDB(collection_name="dnd_test")
    
    # Ã–nce temizle (test iÃ§in)
    if db.collection.count() > 0:
        print("ğŸ—‘ï¸ Eski data temizleniyor...")
        db.clear()
        db = VectorDB(collection_name="dnd_test")
    
    db.add_documents(embedded_docs)
    
    # Test search
    print("\n" + "="*60)
    print("SEARCH TEST")
    print("="*60)
    
    test_queries = [
        "What are ability scores in D&D?",
        "How do I calculate armor class?",
        "What is a saving throw?"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Query: {query}")
        
        # Query'yi embedding'e Ã§evir (aynÄ± model ile!)
        query_embedding = embedder.embed_text(query)
        
        # Embedding ile ara
        results = db.search(query, n_results=3, query_embedding=query_embedding)
        
        print(f"ğŸ“Š En alakalÄ± {len(results)} sonuÃ§:")
        for i, result in enumerate(results, 1):
            print(f"\n{i}. Kaynak: {result['metadata']['source']}")
            print(f"   Chunk: {result['metadata']['chunk_id']}")
            print(f"   Text: {result['text'][:150]}...")
            if result['distance'] is not None:
                print(f"   Distance: {result['distance']:.4f} (dÃ¼ÅŸÃ¼k = iyi)")
            if result['similarity'] is not None:
                print(f"   Similarity: {result['similarity']:.4f} (yÃ¼ksek = iyi)")
    
    # Stats
    print("\n" + "="*60)
    print("DATABASE STATS")
    print("="*60)
    
    stats = db.get_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")
    
    print("\nâœ… Vector database testi baÅŸarÄ±lÄ±!")


if __name__ == "__main__":
    main()
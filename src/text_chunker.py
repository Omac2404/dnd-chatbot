"""
Text chunking modÃ¼lÃ¼
PDF text'ini parÃ§alara bÃ¶ler ve metadata ekler
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter  # type: ignore
from typing import List, Dict
import re


class TextChunker:
    """Text'i anlamlÄ± parÃ§alara bÃ¶len sÄ±nÄ±f"""
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):
        """
        Args:
            chunk_size: Her chunk'Ä±n maksimum karakter sayÄ±sÄ±
            chunk_overlap: Chunk'lar arasÄ± Ã¶rtÃ¼ÅŸme (context sÃ¼rekliliÄŸi iÃ§in)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # RecursiveCharacterTextSplitter: Paragraf > CÃ¼mle > Kelime sÄ±rasÄ±nda bÃ¶ler
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]  # Ã–ncelik sÄ±rasÄ±
        )
    
    def clean_text(self, text: str) -> str:
        """Text'i temizle (geliÅŸtirilmiÅŸ)"""
        # Harf arasÄ± fazla boÅŸluklarÄ± temizle (OCR hatasÄ±)
        text = re.sub(r'([a-z])\s+([a-z])', r'\1\2', text, flags=re.IGNORECASE)
        
        # Kelime arasÄ± Ã§oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        text = re.sub(r'\s+', ' ', text)
        
        # Fazla newline'larÄ± temizle
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # Garip karakterleri temizle
        text = re.sub(r'[^\w\s\.\,\!\?\:\;\-\(\)\[\]\'\"]', '', text)
        
        return text.strip()
    
    def chunk_text(self, text: str, source_name: str = "unknown") -> List[Dict]:
        """
        Text'i chunk'lara bÃ¶l ve metadata ekle
        
        Args:
            text: BÃ¶lÃ¼necek text
            source_name: PDF dosya adÄ± (metadata iÃ§in)
            
        Returns:
            List of dicts with 'text' and 'metadata'
        """
        # Text'i temizle
        cleaned_text = self.clean_text(text)
        
        # Chunk'lara bÃ¶l
        chunks = self.splitter.split_text(cleaned_text)
        
        # Her chunk'a metadata ekle
        chunked_documents = []
        for i, chunk in enumerate(chunks):
            doc = {
                "text": chunk,
                "metadata": {
                    "source": source_name,
                    "chunk_id": i,
                    "total_chunks": len(chunks),
                    "char_count": len(chunk)
                }
            }
            chunked_documents.append(doc)
        
        return chunked_documents
    
    def extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """Text'ten basit keyword extraction (geliÅŸmiÅŸ versiyonlar iÃ§in)"""
        # Basit versiyon: En uzun kelimeler
        words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # En sÄ±k geÃ§en kelimeleri al
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        keywords = [word for word, _ in sorted_words[:max_keywords]]
        
        return keywords


def main():
    """Test scripti"""
    from pdf_processor import extract_text_from_pdf, list_pdfs
    
    print("="*60)
    print("TEXT CHUNKING TEST")
    print("="*60 + "\n")
    
    # PDF'leri listele
    pdfs = list_pdfs()
    
    if not pdfs:
        print("âŒ PDF bulunamadÄ±!")
        return
    
    # Ä°lk PDF'i kullan
    pdf_path = pdfs[0]
    print(f"ğŸ“„ Test PDF: {pdf_path.name}\n")
    
    # Text Ã§Ä±kar
    print("ğŸ“– Text Ã§Ä±karÄ±lÄ±yor...")
    text = extract_text_from_pdf(pdf_path)
    print(f"âœ… Toplam {len(text):,} karakter\n")
    
    # Chunker oluÅŸtur
    chunker = TextChunker(chunk_size=512, chunk_overlap=50)
    
    # Chunk'lara bÃ¶l
    print("âœ‚ï¸ Text chunk'lara bÃ¶lÃ¼nÃ¼yor...")
    chunks = chunker.chunk_text(text, source_name=pdf_path.name)
    print(f"âœ… {len(chunks)} chunk oluÅŸturuldu\n")
    
    # Ä°lk 3 chunk'Ä± gÃ¶ster
    print("="*60)
    print("Ä°LK 3 CHUNK Ã–RNEÄÄ°")
    print("="*60)
    
    for i in range(min(3, len(chunks))):
        chunk = chunks[i]
        print(f"\n--- Chunk {i+1} ---")
        print(f"Kaynak: {chunk['metadata']['source']}")
        print(f"Chunk ID: {chunk['metadata']['chunk_id']}/{chunk['metadata']['total_chunks']}")
        print(f"Karakter: {chunk['metadata']['char_count']}")
        print(f"Ä°Ã§erik: {chunk['text'][:200]}...")
    
    # Ä°statistikler
    print("\n" + "="*60)
    print("CHUNK Ä°STATÄ°STÄ°KLERÄ°")
    print("="*60)
    
    chunk_sizes = [chunk['metadata']['char_count'] for chunk in chunks]
    print(f"Toplam chunk: {len(chunks)}")
    print(f"Ortalama boyut: {sum(chunk_sizes) / len(chunk_sizes):.0f} karakter")
    print(f"Min boyut: {min(chunk_sizes)} karakter")
    print(f"Max boyut: {max(chunk_sizes)} karakter")
    
    print("\nâœ… Text chunking baÅŸarÄ±lÄ±!")


if __name__ == "__main__":
    main()
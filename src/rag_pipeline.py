from typing import List, Dict
from vector_db import VectorDB
from embedder import Embedder
import requests
from anthropic import Anthropic
from config import config


class RAGPipeline:
    """RAG pipeline sÄ±nÄ±fÄ±"""
    
    def __init__(self, use_local_llm: bool = True):
        """
        Args:
            use_local_llm: True -> Llama (local), False -> Claude API
        """
        self.use_local_llm = use_local_llm
        
        # Vector DB
        print("ðŸ“š Vector database yÃ¼kleniyor...")
        self.vector_db = VectorDB(collection_name="dnd_knowledge")
        
        # Ã–NEMLÄ°: Database ile aynÄ± model kullan (768-dim)
        print("ðŸ”¤ Embedding modeli yÃ¼kleniyor...")
        self.embedder = Embedder(model_name="all-mpnet-base-v2")
        
        # Claude client (fallback iÃ§in)
        if not use_local_llm:
            self.claude_client = Anthropic(api_key=config.ANTHROPIC_API_KEY)
        
        print(f"âœ… RAG Pipeline hazÄ±r! LLM: {'Llama (Local)' if use_local_llm else 'Claude API'}")
    
    def retrieve_context(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Query'ye alakalÄ± context'leri vector DB'den al
        
        Args:
            query: KullanÄ±cÄ± sorusu
            top_k: KaÃ§ chunk dÃ¶ndÃ¼rÃ¼lsÃ¼n
            
        Returns:
            En alakalÄ± chunk'lar
        """
        print(f"ðŸ” Retrieval: '{query}' iÃ§in {top_k} chunk aranÄ±yor...")
        
        # Ã–NEMLÄ°: Query'yi embedding'e Ã§evir (aynÄ± model ile!)
        query_embedding = self.embedder.embed_text(query)
        
        # Embedding ile ara
        results = self.vector_db.search(query, n_results=top_k, query_embedding=query_embedding)
        
        print(f"âœ… {len(results)} alakalÄ± chunk bulundu")
        return results
    
    def format_context(self, retrieved_docs: List[Dict]) -> str:
        """Retrieved document'larÄ± prompt iÃ§in formatlÄ± string'e Ã§evir"""
        context_parts = []
        
        for i, doc in enumerate(retrieved_docs, 1):
            source = doc['metadata'].get('source', 'Unknown')
            chunk_id = doc['metadata'].get('chunk_id', 'N/A')
            text = doc['text']
            
            # âœ… YENÄ°: Similarity score ekle (LLM'e hangi source'un daha Ã¶nemli olduÄŸunu gÃ¶sterir)
            similarity = doc.get('similarity', 0.0)
            
            context_parts.append(
                f"[Source {i}: {source}, Chunk {chunk_id}, Relevance: {similarity:.2f}]\n{text}"
            )
        
        return "\n\n---\n\n".join(context_parts)
    
    def generate_with_llama(self, query: str, context: str) -> str:
        """
        Llama (local) ile cevap Ã¼ret
        
        Args:
            query: KullanÄ±cÄ± sorusu
            context: Retrieved context
            
        Returns:
            Llama'nÄ±n cevabÄ±
        """
        # âœ… Ä°YÄ°LEÅžTÄ°RÄ°LMÄ°Åž PROMPT
        prompt = f"""You are a D&D 5th Edition expert assistant. Your job is to answer questions using ONLY the provided context from the Player's Handbook.

INSTRUCTIONS:
1. Answer ONLY based on the provided context
2. If the context doesn't contain enough information, explicitly say: "I don't have enough information in the provided text to answer this question."
3. ALWAYS cite your sources using the format: (Source X: Chunk Y)
4. Be specific and detailed in your answer
5. If multiple sources provide relevant information, combine them coherently

Context from D&D Player's Handbook:
{context}

User Question: {query}

Answer:"""
        
        # Ollama API'ye istek
        url = f"{config.OLLAMA_BASE_URL}/api/generate"
        data = {
            "model": config.OLLAMA_MODEL,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "num_predict": 512  # max_tokens yerine
            }
        }
        
        print("ðŸ¤– Llama ile cevap Ã¼retiliyor...")
        response = requests.post(url, json=data, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            return result['response']
        else:
            raise Exception(f"Ollama API error: {response.status_code}")
    
    def generate_with_claude(self, query: str, context: str) -> str:
        """
        Claude API ile cevap Ã¼ret
        
        Args:
            query: KullanÄ±cÄ± sorusu
            context: Retrieved context
            
        Returns:
            Claude'un cevabÄ±
        """
        prompt = f"""You are a D&D 5th Edition expert assistant. Answer the question using ONLY the provided context.

Context:
{context}

Question: {query}

Provide a clear, accurate answer with source citations in the format (Source X: Chunk Y)."""
        
        print("ðŸ¤– Claude API ile cevap Ã¼retiliyor...")
        message = self.claude_client.messages.create(
            model=config.CLAUDE_MODEL,
            max_tokens=1024,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return message.content[0].text
    
    def query(self, user_question: str, top_k: int = 5) -> Dict:
        """
        RAG pipeline'Ä±n ana fonksiyonu
        
        Args:
            user_question: KullanÄ±cÄ± sorusu
            top_k: KaÃ§ context chunk kullanÄ±lsÄ±n (âœ… Default 5'e Ã§Ä±karÄ±ldÄ±)
            
        Returns:
            Dict with 'answer', 'sources', 'context'
        """
        print("\n" + "="*60)
        print(f"ðŸ“ Soru: {user_question}")
        print("="*60)
        
        # 1. RETRIEVAL
        retrieved_docs = self.retrieve_context(user_question, top_k=top_k)
        
        # 2. FORMAT CONTEXT
        context = self.format_context(retrieved_docs)
        
        # 3. GENERATION
        if self.use_local_llm:
            answer = self.generate_with_llama(user_question, context)
        else:
            answer = self.generate_with_claude(user_question, context)
        
        print("âœ… Cevap Ã¼retildi!")
        
        # SonuÃ§
        result = {
            "question": user_question,
            "answer": answer,
            "sources": [
                {
                    "source": doc['metadata']['source'],
                    "chunk_id": doc['metadata']['chunk_id'],
                    "text_preview": doc['text'][:200],
                    "similarity": doc.get('similarity', 0.0)
                }
                for doc in retrieved_docs
            ],
            "context_used": context
        }
        
        return result
    
    def calculate_confidence(self, answer: str, sources: List[Dict] = None) -> float:
        """
        âœ… Ä°YÄ°LEÅžTÄ°RÄ°LMÄ°Åž: CevabÄ±n confidence skorunu hesapla
        
        Args:
            answer: LLM'in cevabÄ±
            sources: KullanÄ±lan kaynaklar (similarity skorlarÄ± iÃ§in)
        
        Returns:
            0.0 - 1.0 arasÄ± confidence score
        """
        confidence = 1.0  # BaÅŸlangÄ±Ã§: yÃ¼ksek confidence
        
        # 1. DÃ¼ÅŸÃ¼k confidence ifadeleri (aÄŸÄ±rlÄ±k: -0.6)
        low_confidence_phrases = [
            "i don't know",
            "i don't have",
            "not sure",
            "unclear",
            "cannot find",
            "not enough information"
        ]
        
        answer_lower = answer.lower()
        
        for phrase in low_confidence_phrases:
            if phrase in answer_lower:
                confidence -= 0.6
                break
        
        # 2. Uzunluk kontrolÃ¼ (Ã§ok kÄ±sa = ÅŸÃ¼pheli)
        if len(answer) < 50:
            confidence -= 0.3
        
        # 3. Source citation var mÄ±? (Ã¶nemli!)
        if "source" in answer_lower or "chunk" in answer_lower:
            confidence += 0.1  # Kaynak gÃ¶sterdiyse +bonus
        else:
            confidence -= 0.2  # Kaynak gÃ¶stermediyse -ceza
        
        # 4. âœ… YENÄ°: Retrieved sources'larÄ±n avg similarity
        if sources:
            avg_similarity = sum(s.get('similarity', 0.0) for s in sources) / len(sources)
            
            # YÃ¼ksek similarity = yÃ¼ksek confidence
            if avg_similarity > 0.7:
                confidence += 0.1
            elif avg_similarity < 0.4:
                confidence -= 0.2
        
        # 5. SÄ±nÄ±rla 0-1 arasÄ±
        confidence = max(0.0, min(1.0, confidence))
        
        return confidence


def main():
    """RAG Pipeline test scripti"""
    
    print("="*60)
    print("RAG PIPELINE TEST")
    print("="*60 + "\n")
    
    # RAG pipeline oluÅŸtur
    rag = RAGPipeline(use_local_llm=True)  # Llama kullan
    
    # Test sorularÄ±
    test_questions = [
        "What are the six ability scores in D&D 5e?",
        "How do I calculate my armor class?",
        "What is a saving throw and when do I make one?",
        "Explain the difference between a skill check and an ability check.",
    ]
    
    print("\n" + "="*60)
    print("TEST SORULARI")
    print("="*60)
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n\n{'#'*60}")
        print(f"TEST {i}/{len(test_questions)}")
        print(f"{'#'*60}")
        
        # âœ… TOP_K = 5 (3 yerine)
        result = rag.query(question, top_k=5)
        
        # SonuÃ§larÄ± gÃ¶ster
        print("\n" + "-"*60)
        print("CEVAP:")
        print("-"*60)
        print(result['answer'])
        
        print("\n" + "-"*60)
        print("KAYNAKLAR:")
        print("-"*60)
        for j, source in enumerate(result['sources'], 1):
            print(f"\n{j}. {source['source']} (Chunk {source['chunk_id']})")
            print(f"   Similarity: {source['similarity']:.4f}")
            print(f"   Ã–nizleme: {source['text_preview']}...")
        
        # âœ… Ä°yileÅŸtirilmiÅŸ confidence calculation
        confidence = rag.calculate_confidence(result['answer'], result['sources'])
        print(f"\nðŸ“Š Confidence Score: {confidence:.2f}")
        
        if confidence < config.CONFIDENCE_THRESHOLD:
            print("âš ï¸ DÃ¼ÅŸÃ¼k confidence - Web aramasÄ± Ã¶nerilir")
    
    print("\n\n" + "="*60)
    print("âœ… RAG PIPELINE TESTÄ° TAMAMLANDI!")
    print("="*60)


if __name__ == "__main__":
    main()


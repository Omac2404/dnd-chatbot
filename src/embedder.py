"""
Embedding modÃ¼lÃ¼
Text'i vektÃ¶rlere Ã§evirir (semantic search iÃ§in)
"""

from sentence_transformers import SentenceTransformer # type: ignore
import numpy as np # type: ignore
from typing import List, Dict
from tqdm import tqdm # type: ignore


class Embedder:
    """Text embedding sÄ±nÄ±fÄ±"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        """
        Args:
            model_name: KullanÄ±lacak embedding modeli
            
        PopÃ¼ler modeller:
        - all-MiniLM-L6-v2: HÄ±zlÄ±, 384-dim (Ã–NERÄ°LEN)
        - all-mpnet-base-v2: Daha iyi, 768-dim (yavaÅŸ ama daha baÅŸarÄ±lÄ±)
        - paraphrase-MiniLM-L6-v2: Paraphrase detection iÃ§in
        """
        print(f"ğŸ“¥ Embedding modeli yÃ¼kleniyor: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()
        print(f"âœ… Model hazÄ±r! Embedding boyutu: {self.embedding_dim}")
    
    def embed_text(self, text: str) -> np.ndarray:
        """Tek bir text'i embedding'e Ã§evir"""
        return self.model.encode(text, convert_to_numpy=True)
    
    def embed_batch(self, texts: List[str], batch_size: int = 32, show_progress: bool = True) -> np.ndarray:
        """
        Birden fazla text'i batch olarak embedding'e Ã§evir
        
        Args:
            texts: Text listesi
            batch_size: Batch boyutu (GPU yoksa 32 yeterli)
            show_progress: Progress bar gÃ¶ster mi?
            
        Returns:
            (N, embedding_dim) shaped numpy array
        """
        return self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )
    
    def embed_documents(self, documents: List[Dict]) -> List[Dict]:
        """
        Chunk document'larÄ±na embedding ekle
        
        Args:
            documents: text_chunker'dan gelen documents
            
        Returns:
            Her document'a 'embedding' field'Ä± eklenmiÅŸ liste
        """
        texts = [doc['text'] for doc in documents]
        
        print(f"ğŸ”„ {len(texts)} chunk embedding'e Ã§evriliyor...")
        embeddings = self.embed_batch(texts, show_progress=True)
        
        # Embedding'leri document'lara ekle
        for doc, embedding in zip(documents, embeddings):
            doc['embedding'] = embedding
        
        print(f"âœ… Embedding tamamlandÄ±!")
        return documents
    
    def compute_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Ä°ki embedding arasÄ±nda cosine similarity hesapla"""
        # Cosine similarity = dot product / (norm1 * norm2)
        similarity = np.dot(embedding1, embedding2) / (
            np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
        )
        return float(similarity)


def main():
    """Test scripti"""
    from text_chunker import TextChunker
    from pdf_processor import extract_text_from_pdf, list_pdfs
    
    print("="*60)
    print("EMBEDDING TEST")
    print("="*60 + "\n")
    
    # PDF'leri listele
    pdfs = list_pdfs()
    
    if not pdfs:
        print("âŒ PDF bulunamadÄ±!")
        return
    
    pdf_path = pdfs[0]
    print(f"ğŸ“„ Test PDF: {pdf_path.name}\n")
    
    # Text Ã§Ä±kar ve chunk'la (sadece ilk 100 chunk test iÃ§in)
    print("ğŸ“– Text iÅŸleniyor...")
    text = extract_text_from_pdf(pdf_path)
    
    chunker = TextChunker(chunk_size=512, chunk_overlap=50)
    chunks = chunker.chunk_text(text, source_name=pdf_path.name)
    
    # Test iÃ§in sadece ilk 100 chunk
    test_chunks = chunks[:100]
    print(f"âœ… {len(test_chunks)} chunk hazÄ±r (test iÃ§in)\n")
    
    # Embedder oluÅŸtur
    # DAHA Ä°YÄ° MODEL (yavaÅŸ, 768-dim, %10-15 daha iyi)
    embedder = Embedder(model_name="all-mpnet-base-v2")
    print()
    
    # Embedding'leri oluÅŸtur
    embedded_docs = embedder.embed_documents(test_chunks)
    
    # Ä°lk embedding'i gÃ¶ster
    print("\n" + "="*60)
    print("EMBEDDING Ã–RNEÄÄ°")
    print("="*60)
    
    first_doc = embedded_docs[0]
    print(f"Text: {first_doc['text'][:100]}...")
    print(f"Embedding shape: {first_doc['embedding'].shape}")
    print(f"Ä°lk 10 deÄŸer: {first_doc['embedding'][:10]}")
    
    # Similarity testi
    print("\n" + "="*60)
    print("SIMILARITY TEST")
    print("="*60)
    
    # Daha anlamlÄ± test: AynÄ± bÃ¶lÃ¼mdeki chunk'larÄ± karÅŸÄ±laÅŸtÄ±r
    # Ä°lk 10 chunk muhtemelen aynÄ± bÃ¶lÃ¼mde (Introduction)
    emb1 = embedded_docs[5]['embedding']  # 5. chunk
    emb2 = embedded_docs[6]['embedding']  # 6. chunk (hemen yan yana)
    emb3 = embedded_docs[70]['embedding'] # 70. chunk (Ã§ok uzak)

    sim_close = embedder.compute_similarity(emb1, emb2)
    sim_far = embedder.compute_similarity(emb1, emb3)

    print(f"Chunk 5 <-> Chunk 6 similarity: {sim_close:.4f} (yan yana chunk'lar)")
    print(f"Chunk 5 <-> Chunk 70 similarity: {sim_far:.4f} (uzak chunk'lar)")
    
    sim_close = embedder.compute_similarity(emb1, emb2)
    sim_far = embedder.compute_similarity(emb1, emb3)
    
    print(f"Chunk 0 <-> Chunk 1 similarity: {sim_close:.4f} (yakÄ±n chunk'lar)")
    print(f"Chunk 0 <-> Chunk 50 similarity: {sim_far:.4f} (uzak chunk'lar)")
    print("\nğŸ’¡ YakÄ±n chunk'lar daha yÃ¼ksek similarity'e sahip olmalÄ±!")
    
    print("\nâœ… Embedding testi baÅŸarÄ±lÄ±!")


if __name__ == "__main__":
    main()